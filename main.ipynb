{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (4.43.1)\n",
      "Requirement already satisfied: filelock in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from langchain-community) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Downloading SQLAlchemy-2.0.31-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.9 (from langchain-community)\n",
      "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from langchain-community) (8.5.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from langchain-core) (24.1)\n",
      "Collecting pydantic<3,>=1 (from langchain-core)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.9->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading orjson-3.10.6-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain-core)\n",
      "  Downloading pydantic_core-2.20.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp39-cp39-macosx_11_0_arm64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.7/390.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.31-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading orjson-3.10.6-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: SQLAlchemy, pydantic-core, orjson, mypy-extensions, multidict, marshmallow, jsonpointer, frozenlist, async-timeout, annotated-types, yarl, typing-inspect, pydantic, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.4.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.23 langchain-text-splitters-0.2.2 langsmith-0.1.93 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.6 pydantic-2.8.2 pydantic-core-2.20.1 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from pypdf) (4.12.2)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (0.24.1)\n",
      "Requirement already satisfied: filelock in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (71.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader, UnstructuredFileLoader \n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_PATH = '/Users/janekkorczynski/CVExtraction/cvs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CV_JanKorczynski_2024.pdf\n",
      "1 Julie Plink Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(CV_PATH)\n",
    "for e, v in enumerate(files):\n",
    "        print(e, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inf(i):\n",
    "    s = time.time()\n",
    "    try: \n",
    "        text = load_docs(CV_PATH + i)               # Loading the file from folder\n",
    "        if len(text) < 100: return \"file issue\"\n",
    "        data = langchain.run(context)\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(\"Cannot Extract Details\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Information From PDF and DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path):\n",
    "    doc_vec = []\n",
    "    try: \n",
    "        if '.pdf' in path: \n",
    "            pdf = PyPDFLoader(path)\n",
    "            pdf_p = pdf.load_and_split()\n",
    "            text = ''\n",
    "            for page in pdf_p:\n",
    "                text += page.page_content\n",
    "            return text \n",
    "        else:\n",
    "            loader = Docx2txtLoader(path)\n",
    "            doc_d = loader.load_and_split()\n",
    "            text = \"\"\n",
    "            for page in doc_d:\n",
    "                text += page.page_content\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'cannot parse -- {path}')                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_model(model_name, temperature=0.7, max_tokens=150, top_p=0.9, frequency_penalty=0.0, model_kwargs = {\n",
    "    'stop': [\"**Note**\", \"**Please**\", \"**Please note: **\" ]\n",
    "}):\n",
    "    \"\"\"\n",
    "    Load the text generation pipeline with the specified model and parameters.\n",
    "\n",
    "    Parameters:\n",
    "    model_name (str): The name of the model to load.\n",
    "    temperature (float): Sampling temperature.\n",
    "    max_tokens (int): Maximum number of tokens to generate.\n",
    "    top_p (float): Nucleus sampling parameter.\n",
    "    frequency_penalty (float): Frequency penalty parameter.\n",
    "\n",
    "    Returns:\n",
    "    pipeline: The loaded text generation pipeline.\n",
    "    \"\"\"\n",
    "    pipe = pipeline(\"text-classification\", model=model_name)\n",
    "    return lambda prompt: pipe(prompt, max_length=max_tokens, temperature=temperature, top_p=top_p, frequency_penalty=frequency_penalty, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llama_text = \"\"\"\n",
    "You are a recruiter for a company, and your task is to extract all essential information provided by a potential employee in their resume.\n",
    "Differentiate clearly between branch and degree. \n",
    "\n",
    "- \"degree\" refers to the specific academic qualification obtained (e.g., Bachelor of Engineering, Master of Science).\n",
    "- \"branch\" refers to the specific field of study within the degree (e.g., Computer Engineering, Mechanical Engineering).\n",
    "\n",
    "Ensure that all abbreviations are expanded to their full forms in the JSON output. Use acronyms only if you do not know the full form.\n",
    "\n",
    "The start date and end date should be in the format MM/YYYY.\n",
    "All extracted information should be formatted in JSON format:\n",
    "\n",
    "{\n",
    "    \"name\": \"\",\n",
    "    \"phone_number\": \"\", \n",
    "    \"email_address\": \"\",\n",
    "    \"date_of_birth\": \"\", \n",
    "\n",
    "    \"education\" : [\n",
    "        {\n",
    "            \"degree\": \"\", \n",
    "            \"branch\": \"\", \n",
    "            \"university\": \"\", \n",
    "            \"graduation_date\": \"\"\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"work_experience\": [\n",
    "        {\n",
    "            \"company_name\": \"\", \n",
    "            \"job_title\": \"\", \n",
    "            \"start_date\": \"\", \n",
    "            \"end_date\": \"\"\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"total_years_of_experience\": \"end_date - start_date\",\n",
    "    \"technical_skills\" : [],\n",
    "    \"soft_skills\": []\n",
    "}\n",
    "\n",
    "Extracted CV text: \n",
    "```{cv_text}```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cv_info(cv_text, model_name=\"meta-llama/Prompt-Guard-86M\", temperature=0.7, max_tokens=150, top_p=0.9, frequency_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Extract important information from a CV using the specified model and parameters.\n",
    "    \"\"\"\n",
    "    pipe = load_model(model_name, temperature, max_tokens, top_p, frequency_penalty)\n",
    "    prompt = prompt_llama_text.replace(\"{cv_text}\", cv_text)\n",
    "    result = pipe(prompt)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|          | 0/2 [00:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     cv_text \u001b[38;5;241m=\u001b[39m load_docs(file_path)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv_text:\n\u001b[0;32m----> 7\u001b[0m         extracted_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_cv_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_json\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_info})\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m, in \u001b[0;36mextract_cv_info\u001b[0;34m(cv_text, model_name, temperature, max_tokens, top_p, frequency_penalty)\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m load_model(model_name, temperature, max_tokens, top_p, frequency_penalty)\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_llama_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{cv_text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cv_text)\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[72], line 22\u001b[0m, in \u001b[0;36mload_model.<locals>.<lambda>\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mLoad the text generation pipeline with the specified model and parameters.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mpipeline: The loaded text generation pipeline.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m prompt: \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 156\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/pipelines/base.py:1254\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         )\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/pipelines/base.py:1260\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1260\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:180\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3073\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3181\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   3161\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3162\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3178\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3179\u001b[0m     )\n\u001b[1;32m   3180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3184\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3247\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3248\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3253\u001b[0m )\n\u001b[0;32m-> 3255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3258\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    580\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    599\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    600\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 601\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'temperature'"
     ]
    }
   ],
   "source": [
    "# Process all files and save results\n",
    "results = []\n",
    "for file_name in tqdm(files):\n",
    "    file_path = os.path.join(CV_PATH, file_name)\n",
    "    cv_text = load_docs(file_path)\n",
    "    if cv_text:\n",
    "        extracted_info = extract_cv_info(cv_text)\n",
    "        results.append({\"file_name\": file_name, \"extracted_json\": extracted_info})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('extracted_cv_information.csv', index=False)\n",
    "\n",
    "print(\"Extracted information saved to extracted_cv_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in analyzing and understanding clinical doctor notes in healthcare.\n",
    "Extract all symptoms only from the clinical note below in triple backticks.\n",
    "Differentiate between symptoms that are present vs. absent.\n",
    "Give me the probability (high/ medium/ low) of how sure you are about the result.\n",
    "Add a note on the probabilities and why you think so.\n",
    "Output as a markdown table with the following columns,\n",
    "all symptoms should be expanded and no acronyms unless you don't know:\n",
    "Symptoms | Present/Denies | Probability.\n",
    "Also expand the acronyms in the note including symptoms and other medical terms.\n",
    "Do not leave out any acronym related to healthcare.\n",
    "Output that also as a separate appendix table in Markdown with the following columns,\n",
    "Acronym | Expanded Term\n",
    "Clinical Note:\n",
    "```{clinical_note}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment: JULIE PLINK \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.993646502494812, 'text': 'JULIE PLINK '}]\n",
      "Segment:  jcp332@cornell.edu • julie.plink@gmail.com • (914) 319-6351 •  LinkedIn \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9964776635169983, 'text': ' jcp332@cornell.edu • julie.plink@gmail.com • (914) 319-6351 •  LinkedIn '}]\n",
      "Segment:  SUM M ARY \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.46122926473617554, 'text': ' SUM M ARY '}]\n",
      "Segment:  A globally experienced Cornell Engineering student with dual US and EU citizenship. Highly motivated and energetic. Keen interest \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9884695410728455, 'text': ' A globally experienced Cornell Engineering student with dual US and EU citizenship. Highly motivated and energetic. Keen interest '}]\n",
      "Segment:  in the technology and finance industries, and recipient of a selective Cornell Engineering scholarship. Pursuing a Computer Science \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.8762067556381226, 'text': ' in the technology and finance industries, and recipient of a selective Cornell Engineering scholarship. Pursuing a Computer Science '}]\n",
      "Segment:  degree, with minors in Applied Economics and Operations Research & Information Engineering. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9861214756965637, 'text': ' degree, with minors in Applied Economics and Operations Research & Information Engineering. '}]\n",
      "Segment:  EDUCATION \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9981912970542908, 'text': ' EDUCATION '}]\n",
      "Segment:  Cornell University  Ithaca, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9899182915687561, 'text': ' Cornell University  Ithaca, New York '}]\n",
      "Segment:  B.S. in Computer Science / College of Engineering  Expected  May 2025 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.8747451901435852, 'text': ' B.S. in Computer Science / College of Engineering  Expected  May 2025 '}]\n",
      "Segment:  Relevant Coursework  GPA: 3.350/4.000 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.5939292907714844, 'text': ' Relevant Coursework  GPA: 3.350/4.000 '}]\n",
      "Segment:  ●  Analysis of Algorithms, Discrete Structures  ,  Functional  Programming  &  Data Structures  ,  Object  Oriented  Programming  &  Data \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.8724713921546936, 'text': ' ●  Analysis of Algorithms, Discrete Structures  ,  Functional  Programming  &  Data Structures  ,  Object  Oriented  Programming  &  Data '}]\n",
      "Segment:  Structures  ,  Engineering Probability & Statistics, Optimization 1, Intro to Computing, Linear Algebra, Multivariable Calculus, \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9873762130737305, 'text': ' Structures  ,  Engineering Probability & Statistics, Optimization 1, Intro to Computing, Linear Algebra, Multivariable Calculus, '}]\n",
      "Segment:  Differential Calculus, Financial & Managerial Accounting, Intermediate Microeconomics, Intro Microeconomics, Intro \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9580292105674744, 'text': ' Differential Calculus, Financial & Managerial Accounting, Intermediate Microeconomics, Intro Microeconomics, Intro '}]\n",
      "Segment:  Macroeconomics \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.986423671245575, 'text': ' Macroeconomics '}]\n",
      "Segment:  Fordham University  Bronx, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9872850775718689, 'text': ' Fordham University  Bronx, New York '}]\n",
      "Segment:  Course in Physics II: Electricity and Magnetism  July 2022 - August 2022 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9899039268493652, 'text': ' Course in Physics II: Electricity and Magnetism  July 2022 - August 2022 '}]\n",
      "Segment:  Rye High School  Rye, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.8525700569152832, 'text': ' Rye High School  Rye, New York '}]\n",
      "Segment:  High School Diploma  Received June 2021 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.544269323348999, 'text': ' High School Diploma  Received June 2021 '}]\n",
      "Segment:  Relevant Coursework  GPA: 98.67/100 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.503516435623169, 'text': ' Relevant Coursework  GPA: 98.67/100 '}]\n",
      "Segment:  ●  AP Calculus, AP CS Principles, AP Physics 1, Intro to Engineering & Design, Principles of Engineering, Robotics, Economics \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.7338553071022034, 'text': ' ●  AP Calculus, AP CS Principles, AP Physics 1, Intro to Engineering & Design, Principles of Engineering, Robotics, Economics '}]\n",
      "Segment:  WORK EXPERIENCE \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.5720188617706299, 'text': ' WORK EXPERIENCE '}]\n",
      "Segment:  Casetext  Virtual \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9830157160758972, 'text': ' Casetext  Virtual '}]\n",
      "Segment:  Software Engineering Intern, Special Projects  June 2023 - August 2023 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9509392380714417, 'text': ' Software Engineering Intern, Special Projects  June 2023 - August 2023 '}]\n",
      "Segment:  ●  Utilized GPT-4 for prompt engineering, enhancing legal research methodologies by optimizing existing enterprise projects. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9894379377365112, 'text': ' ●  Utilized GPT-4 for prompt engineering, enhancing legal research methodologies by optimizing existing enterprise projects. '}]\n",
      "Segment:  ●  Managed and delivered tailored enterprise solutions for key clients, leveraging advancements in AI for project execution. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9893934726715088, 'text': ' ●  Managed and delivered tailored enterprise solutions for key clients, leveraging advancements in AI for project execution. '}]\n",
      "Segment:  CS 3110: Functional Programming and Data Structures  Cornell University - Ithaca, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9756046533584595, 'text': ' CS 3110: Functional Programming and Data Structures  Cornell University - Ithaca, New York '}]\n",
      "Segment:  Consultant  August 2023 - December 2023 \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.6633468270301819, 'text': ' Consultant  August 2023 - December 2023 '}]\n",
      "Segment:  ●  Facilitate weekly office hours, assist with assignment and exam grading, collaborate in design of course assignments. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9999293088912964, 'text': ' ●  Facilitate weekly office hours, assist with assignment and exam grading, collaborate in design of course assignments. '}]\n",
      "Segment:  ●  Serve as project manager for 2 student groups to assist on completion of the semester long final project. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.999427318572998, 'text': ' ●  Serve as project manager for 2 student groups to assist on completion of the semester long final project. '}]\n",
      "Segment:  Albert Einstein College of Medicine  Bronx, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.990196943283081, 'text': ' Albert Einstein College of Medicine  Bronx, New York '}]\n",
      "Segment:  Image Analysis Assistant Intern  May  2020 - July 2020 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9970723390579224, 'text': ' Image Analysis Assistant Intern  May  2020 - July 2020 '}]\n",
      "Segment:  ●  Explored breast cancer mortality disparities using novel image techniques and analyzing cell stains with existing algorithms. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9986057877540588, 'text': ' ●  Explored breast cancer mortality disparities using novel image techniques and analyzing cell stains with existing algorithms. '}]\n",
      "Segment:  CLUBS AND ORGANIZATIONS \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9567097425460815, 'text': ' CLUBS AND ORGANIZATIONS '}]\n",
      "Segment:  Cornell DEBUT  -  Biomedical Engineering Project Team  Cornell University - Ithaca, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9960896968841553, 'text': ' Cornell DEBUT  -  Biomedical Engineering Project Team  Cornell University - Ithaca, New York '}]\n",
      "Segment:  Operations Team Lead  August 2023 - Present \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9400351643562317, 'text': ' Operations Team Lead  August 2023 - Present '}]\n",
      "Segment:  ●  Lead 7 person Operations subteam; coordinate team logistics, sponsorship, and social media. Direct workshops for 60+ \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9958178400993347, 'text': ' ●  Lead 7 person Operations subteam; coordinate team logistics, sponsorship, and social media. Direct workshops for 60+ '}]\n",
      "Segment:  members and manage a $20,000+ budget, ensuring efficient procurement of materials for project development. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9998997449874878, 'text': ' members and manage a $20,000+ budget, ensuring efficient procurement of materials for project development. '}]\n",
      "Segment:  Operations Analyst  October 2021 - August 2023 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.8920854926109314, 'text': ' Operations Analyst  October 2021 - August 2023 '}]\n",
      "Segment:  ●  Liaison between R&D teams and Operations team; collaborate on sponsorship outreach, social media and website management. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.8140883445739746, 'text': ' ●  Liaison between R&D teams and Operations team; collaborate on sponsorship outreach, social media and website management. '}]\n",
      "Segment:  Cornell University Women’s Division I Rowing Team  Cornell University - Ithaca, New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.6387680768966675, 'text': ' Cornell University Women’s Division I Rowing Team  Cornell University - Ithaca, New York '}]\n",
      "Segment:  Varsity Athlete  November 2022 - November 2023 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9939261078834534, 'text': ' Varsity Athlete  November 2022 - November 2023 '}]\n",
      "Segment:  ●  Demonstrated personal accountability by meeting rigorous physical and mental training standards; collaborating with \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9906617999076843, 'text': ' ●  Demonstrated personal accountability by meeting rigorous physical and mental training standards; collaborating with '}]\n",
      "Segment:  teammates to optimize performance. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9997863173484802, 'text': ' teammates to optimize performance. '}]\n",
      "Segment:  ●  Showcased time management skills by balancing a demanding academic schedule with rigorous training and competition. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9980321526527405, 'text': ' ●  Showcased time management skills by balancing a demanding academic schedule with rigorous training and competition. '}]\n",
      "Segment:  Research in Operations Research and Engineering  Cornell University - Ithaca,  New York \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.970051646232605, 'text': ' Research in Operations Research and Engineering  Cornell University - Ithaca,  New York '}]\n",
      "Segment:  Analyst - Data Science for Cornell  January 2023 - May 2023 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.992841362953186, 'text': ' Analyst - Data Science for Cornell  January 2023 - May 2023 '}]\n",
      "Segment:  ●  Collaboratively identified and tackled healthcare accessibility issues on campus, developing viable solutions and liaising with \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9516000747680664, 'text': ' ●  Collaboratively identified and tackled healthcare accessibility issues on campus, developing viable solutions and liaising with '}]\n",
      "Segment:  industry professionals for potential project expansion in future semesters. \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.5878093838691711, 'text': ' industry professionals for potential project expansion in future semesters. '}]\n",
      "Segment:  Analyst - Simulation Analysis for CVS Health Supply Chain  August 2022  - December 2022 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9982403516769409, 'text': ' Analyst - Simulation Analysis for CVS Health Supply Chain  August 2022  - December 2022 '}]\n",
      "Segment:  ●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives.●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives. \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.8327417373657227, 'text': ' ●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives.●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives. '}]\n",
      "Segment:  Analyst - Strategic Pricing for General Motors  August 2022 - December 2022 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.985754132270813, 'text': ' Analyst - Strategic Pricing for General Motors  August 2022 - December 2022 '}]\n",
      "Segment:  ●  Worked with Cornell ORACL & GM on causal inference studies in advertising/pricing, conducting reviews and analyses. \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.5983832478523254, 'text': ' ●  Worked with Cornell ORACL & GM on causal inference studies in advertising/pricing, conducting reviews and analyses. '}]\n",
      "Segment:  HONORS AND AWARDS \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.5933499932289124, 'text': ' HONORS AND AWARDS '}]\n",
      "Segment:  Joan and Irwin Jacobs Scholar at Cornell University College of Engineering  December 2020 - Present \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9988289475440979, 'text': ' Joan and Irwin Jacobs Scholar at Cornell University College of Engineering  December 2020 - Present '}]\n",
      "Segment:  Rye Youth Council Award  Awarded  June 2021 \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9589231610298157, 'text': ' Rye Youth Council Award  Awarded  June 2021 '}]\n",
      "Segment:  National Honor Society Member  September 2017 - June 2021 \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9289159774780273, 'text': ' National Honor Society Member  September 2017 - June 2021 '}]\n",
      "Segment:  SKILLS & LANGUAGES \n",
      "Classification: [{'label': 'BENIGN', 'score': 0.9271197319030762, 'text': ' SKILLS & LANGUAGES '}]\n",
      "Segment:  Programming & Software:  HTML, CSS, Java, Python, OCaml, Microsoft Office, Git \n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9991257786750793, 'text': ' Programming & Software:  HTML, CSS, Java, Python, OCaml, Microsoft Office, Git '}]\n",
      "Segment:  Languages:  English and Dutch (Fluent); French (Conversational)\n",
      "Classification: [{'label': 'INJECTION', 'score': 0.9977850914001465, 'text': ' Languages:  English and Dutch (Fluent); French (Conversational)'}]\n",
      "Classification Label: BENIGN, Text: JULIE PLINK \n",
      "Classification Label: BENIGN, Text:  jcp332@cornell.edu • julie.plink@gmail.com • (914) 319-6351 •  LinkedIn \n",
      "Classification Label: BENIGN, Text:  SUM M ARY \n",
      "Classification Label: INJECTION, Text:  A globally experienced Cornell Engineering student with dual US and EU citizenship. Highly motivated and energetic. Keen interest \n",
      "Classification Label: INJECTION, Text:  in the technology and finance industries, and recipient of a selective Cornell Engineering scholarship. Pursuing a Computer Science \n",
      "Classification Label: INJECTION, Text:  degree, with minors in Applied Economics and Operations Research & Information Engineering. \n",
      "Classification Label: BENIGN, Text:  EDUCATION \n",
      "Classification Label: BENIGN, Text:  Cornell University  Ithaca, New York \n",
      "Classification Label: INJECTION, Text:  B.S. in Computer Science / College of Engineering  Expected  May 2025 \n",
      "Classification Label: INJECTION, Text:  Relevant Coursework  GPA: 3.350/4.000 \n",
      "Classification Label: INJECTION, Text:  ●  Analysis of Algorithms, Discrete Structures  ,  Functional  Programming  &  Data Structures  ,  Object  Oriented  Programming  &  Data \n",
      "Classification Label: INJECTION, Text:  Structures  ,  Engineering Probability & Statistics, Optimization 1, Intro to Computing, Linear Algebra, Multivariable Calculus, \n",
      "Classification Label: INJECTION, Text:  Differential Calculus, Financial & Managerial Accounting, Intermediate Microeconomics, Intro Microeconomics, Intro \n",
      "Classification Label: BENIGN, Text:  Macroeconomics \n",
      "Classification Label: BENIGN, Text:  Fordham University  Bronx, New York \n",
      "Classification Label: INJECTION, Text:  Course in Physics II: Electricity and Magnetism  July 2022 - August 2022 \n",
      "Classification Label: BENIGN, Text:  Rye High School  Rye, New York \n",
      "Classification Label: INJECTION, Text:  High School Diploma  Received June 2021 \n",
      "Classification Label: INJECTION, Text:  Relevant Coursework  GPA: 98.67/100 \n",
      "Classification Label: BENIGN, Text:  ●  AP Calculus, AP CS Principles, AP Physics 1, Intro to Engineering & Design, Principles of Engineering, Robotics, Economics \n",
      "Classification Label: INJECTION, Text:  WORK EXPERIENCE \n",
      "Classification Label: BENIGN, Text:  Casetext  Virtual \n",
      "Classification Label: INJECTION, Text:  Software Engineering Intern, Special Projects  June 2023 - August 2023 \n",
      "Classification Label: INJECTION, Text:  ●  Utilized GPT-4 for prompt engineering, enhancing legal research methodologies by optimizing existing enterprise projects. \n",
      "Classification Label: INJECTION, Text:  ●  Managed and delivered tailored enterprise solutions for key clients, leveraging advancements in AI for project execution. \n",
      "Classification Label: BENIGN, Text:  CS 3110: Functional Programming and Data Structures  Cornell University - Ithaca, New York \n",
      "Classification Label: BENIGN, Text:  Consultant  August 2023 - December 2023 \n",
      "Classification Label: INJECTION, Text:  ●  Facilitate weekly office hours, assist with assignment and exam grading, collaborate in design of course assignments. \n",
      "Classification Label: INJECTION, Text:  ●  Serve as project manager for 2 student groups to assist on completion of the semester long final project. \n",
      "Classification Label: BENIGN, Text:  Albert Einstein College of Medicine  Bronx, New York \n",
      "Classification Label: INJECTION, Text:  Image Analysis Assistant Intern  May  2020 - July 2020 \n",
      "Classification Label: INJECTION, Text:  ●  Explored breast cancer mortality disparities using novel image techniques and analyzing cell stains with existing algorithms. \n",
      "Classification Label: INJECTION, Text:  CLUBS AND ORGANIZATIONS \n",
      "Classification Label: BENIGN, Text:  Cornell DEBUT  -  Biomedical Engineering Project Team  Cornell University - Ithaca, New York \n",
      "Classification Label: BENIGN, Text:  Operations Team Lead  August 2023 - Present \n",
      "Classification Label: INJECTION, Text:  ●  Lead 7 person Operations subteam; coordinate team logistics, sponsorship, and social media. Direct workshops for 60+ \n",
      "Classification Label: INJECTION, Text:  members and manage a $20,000+ budget, ensuring efficient procurement of materials for project development. \n",
      "Classification Label: INJECTION, Text:  Operations Analyst  October 2021 - August 2023 \n",
      "Classification Label: INJECTION, Text:  ●  Liaison between R&D teams and Operations team; collaborate on sponsorship outreach, social media and website management. \n",
      "Classification Label: BENIGN, Text:  Cornell University Women’s Division I Rowing Team  Cornell University - Ithaca, New York \n",
      "Classification Label: INJECTION, Text:  Varsity Athlete  November 2022 - November 2023 \n",
      "Classification Label: INJECTION, Text:  ●  Demonstrated personal accountability by meeting rigorous physical and mental training standards; collaborating with \n",
      "Classification Label: INJECTION, Text:  teammates to optimize performance. \n",
      "Classification Label: INJECTION, Text:  ●  Showcased time management skills by balancing a demanding academic schedule with rigorous training and competition. \n",
      "Classification Label: BENIGN, Text:  Research in Operations Research and Engineering  Cornell University - Ithaca,  New York \n",
      "Classification Label: INJECTION, Text:  Analyst - Data Science for Cornell  January 2023 - May 2023 \n",
      "Classification Label: INJECTION, Text:  ●  Collaboratively identified and tackled healthcare accessibility issues on campus, developing viable solutions and liaising with \n",
      "Classification Label: BENIGN, Text:  industry professionals for potential project expansion in future semesters. \n",
      "Classification Label: INJECTION, Text:  Analyst - Simulation Analysis for CVS Health Supply Chain  August 2022  - December 2022 \n",
      "Classification Label: BENIGN, Text:  ●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives.●  Researched RFID usage to mitigate inventory loss, and coordinated with CVS Health reps on model progress and objectives. \n",
      "Classification Label: INJECTION, Text:  Analyst - Strategic Pricing for General Motors  August 2022 - December 2022 \n",
      "Classification Label: INJECTION, Text:  ●  Worked with Cornell ORACL & GM on causal inference studies in advertising/pricing, conducting reviews and analyses. \n",
      "Classification Label: BENIGN, Text:  HONORS AND AWARDS \n",
      "Classification Label: INJECTION, Text:  Joan and Irwin Jacobs Scholar at Cornell University College of Engineering  December 2020 - Present \n",
      "Classification Label: BENIGN, Text:  Rye Youth Council Award  Awarded  June 2021 \n",
      "Classification Label: INJECTION, Text:  National Honor Society Member  September 2017 - June 2021 \n",
      "Classification Label: BENIGN, Text:  SKILLS & LANGUAGES \n",
      "Classification Label: INJECTION, Text:  Programming & Software:  HTML, CSS, Java, Python, OCaml, Microsoft Office, Git \n",
      "Classification Label: INJECTION, Text:  Languages:  English and Dutch (Fluent); French (Conversational)\n",
      "Formatted JSON: {'name': '', 'phone_number': '', 'email_address': '', 'date_of_birth': '', 'education': [], 'work_experience': [], 'total_years_of_experience': '', 'technical_skills': [], 'soft_skills': []}\n",
      "Extracted information saved to extracted_cv_information.csv\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "CV_PATH = '/Users/janekkorczynski/CVExtraction/cvs/Julie Plink Resume.pdf'\n",
    "\n",
    "# Load the text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "def load_docs(path):\n",
    "    \"\"\"\n",
    "    Load and extract text content from PDF or DOCX files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if path.endswith('.pdf'):\n",
    "            pdf = PyPDFLoader(path)\n",
    "            pdf_p = pdf.load_and_split()\n",
    "            text = ''.join(page.page_content for page in pdf_p)\n",
    "            return text\n",
    "        elif path.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(path)\n",
    "            doc_d = loader.load_and_split()\n",
    "            text = ''.join(page.page_content for page in doc_d)\n",
    "            return text\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {path}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Cannot parse -- {path}')\n",
    "        return \"\"\n",
    "\n",
    "def classify_text_segments(text):\n",
    "    \"\"\"\n",
    "    Classify text segments using the provided text classification model.\n",
    "    \"\"\"\n",
    "    segments = text.split('\\n')  # Split the text into segments (e.g., by line)\n",
    "    classifications = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            classification = classifier(segment)\n",
    "            if classification:\n",
    "                classification[0]['text'] = segment  # Add the segment text to the classification result\n",
    "                classifications.append(classification[0])\n",
    "                print(f\"Segment: {segment}\")\n",
    "                print(f\"Classification: {classification}\")\n",
    "    return classifications\n",
    "\n",
    "def map_labels_to_json(label):\n",
    "    \"\"\"\n",
    "    Map the model's classification labels to the desired JSON fields.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'NAME': 'name',\n",
    "        'PHONE_NUMBER': 'phone_number',\n",
    "        'EMAIL_ADDRESS': 'email_address',\n",
    "        'DATE_OF_BIRTH': 'date_of_birth',\n",
    "        'DEGREE': 'education',\n",
    "        'BRANCH': 'education',\n",
    "        'UNIVERSITY': 'education',\n",
    "        'GRADUATION_DATE': 'education',\n",
    "        'COMPANY_NAME': 'work_experience',\n",
    "        'JOB_TITLE': 'work_experience',\n",
    "        'START_DATE': 'work_experience',\n",
    "        'END_DATE': 'work_experience',\n",
    "        'TECHNICAL_SKILL': 'technical_skills',\n",
    "        'SOFT_SKILL': 'soft_skills'\n",
    "    }\n",
    "    return mapping.get(label, None)\n",
    "\n",
    "def format_to_json(classifications):\n",
    "    \"\"\"\n",
    "    Format classified text into the desired JSON structure.\n",
    "    \"\"\"\n",
    "    json_result = {\n",
    "        \"name\": \"\",\n",
    "        \"phone_number\": \"\", \n",
    "        \"email_address\": \"\",\n",
    "        \"date_of_birth\": \"\", \n",
    "        \"education\" : [],\n",
    "        \"work_experience\": [],\n",
    "        \"total_years_of_experience\": \"\",\n",
    "        \"technical_skills\" : [],\n",
    "        \"soft_skills\": []\n",
    "    }\n",
    "\n",
    "    for classification in classifications:\n",
    "        label = classification['label']\n",
    "        text = classification['text']\n",
    "        \n",
    "        json_field = map_labels_to_json(label)\n",
    "        if json_field:\n",
    "            if json_field in ['education', 'work_experience']:\n",
    "                # Append to list fields\n",
    "                if json_field == 'education':\n",
    "                    entry = {\n",
    "                        \"degree\": text if label == 'DEGREE' else \"\",\n",
    "                        \"branch\": text if label == 'BRANCH' else \"\",\n",
    "                        \"university\": text if label == 'UNIVERSITY' else \"\",\n",
    "                        \"graduation_date\": text if label == 'GRADUATION_DATE' else \"\"\n",
    "                    }\n",
    "                    json_result['education'].append(entry)\n",
    "                elif json_field == 'work_experience':\n",
    "                    entry = {\n",
    "                        \"company_name\": text if label == 'COMPANY_NAME' else \"\",\n",
    "                        \"job_title\": text if label == 'JOB_TITLE' else \"\",\n",
    "                        \"start_date\": text if label == 'START_DATE' else \"\",\n",
    "                        \"end_date\": text if label == 'END_DATE' else \"\"\n",
    "                    }\n",
    "                    json_result['work_experience'].append(entry)\n",
    "            else:\n",
    "                json_result[json_field] = text\n",
    "\n",
    "    # Calculate total years of experience if dates are available\n",
    "    if json_result['work_experience']:\n",
    "        for work in json_result['work_experience']:\n",
    "            if work['start_date'] and work['end_date']:\n",
    "                start_date = pd.to_datetime(work['start_date'], format='%m/%Y')\n",
    "                end_date = pd.to_datetime(work['end_date'], format='%m/%Y')\n",
    "                work['total_years_of_experience'] = (end_date - start_date).days / 365.25\n",
    "\n",
    "    return json_result\n",
    "\n",
    "# Process the provided CV file\n",
    "file_path = CV_PATH\n",
    "cv_text = load_docs(file_path)\n",
    "if cv_text:\n",
    "    classifications = classify_text_segments(cv_text)\n",
    "    # Debugging: Print out classifications\n",
    "    for classification in classifications:\n",
    "        print(f\"Classification Label: {classification['label']}, Text: {classification['text']}\")\n",
    "        \n",
    "    formatted_json = format_to_json(classifications)\n",
    "    print(f\"Formatted JSON: {formatted_json}\")  # Debugging: Print the formatted JSON\n",
    "    results = [{\"file_name\": os.path.basename(file_path), \"extracted_json\": json.dumps(formatted_json)}]\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('extracted_cv_information.csv', index=False)\n",
    "\n",
    "    print(\"Extracted information saved to extracted_cv_information.csv\")\n",
    "else:\n",
    "    print(\"Failed to extract text from the provided CV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janekkorczynski/.pyenv/versions/3.9.18/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.05s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "def load_docs(path):\n",
    "    \"\"\"\n",
    "    Load and extract text content from PDF or DOCX files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if path.endswith('.pdf'):\n",
    "            pdf = PyPDFLoader(path)\n",
    "            pdf_p = pdf.load_and_split()\n",
    "            text = ''.join(page.page_content for page in pdf_p)\n",
    "            return text\n",
    "        elif path.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(path)\n",
    "            doc_d = loader.load_and_split()\n",
    "            text = ''.join(page.page_content for page in doc_d)\n",
    "            return text\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {path}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Cannot parse -- {path}')\n",
    "        return \"\"\n",
    "\n",
    "def extract_information(cv_text):\n",
    "    \"\"\"\n",
    "    Extract information from CV text using a generative model.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly intelligent system tasked with extracting all relevant information from a resume. \n",
    "    Please provide the extracted details in the following JSON format:\n",
    "    {{\n",
    "        \"name\": \"\",\n",
    "        \"phone_number\": \"\",\n",
    "        \"email_address\": \"\",\n",
    "        \"date_of_birth\": \"\", \n",
    "        \"education\": [\n",
    "            {{\n",
    "                \"degree\": \"\",\n",
    "                \"branch\": \"\",\n",
    "                \"university\": \"\",\n",
    "                \"graduation_date\": \"\"\n",
    "            }}\n",
    "        ],\n",
    "        \"work_experience\": [\n",
    "            {{\n",
    "                \"company_name\": \"\",\n",
    "                \"job_title\": \"\",\n",
    "                \"start_date\": \"\",\n",
    "                \"end_date\": \"\"\n",
    "            }}\n",
    "        ],\n",
    "        \"total_years_of_experience\": \"\",\n",
    "        \"technical_skills\": [],\n",
    "        \"soft_skills\": []\n",
    "    }}\n",
    "    \n",
    "    Extracted CV text:\n",
    "    ```{cv_text}```\n",
    "    \"\"\"\n",
    "    response = generator(prompt, max_length=1500, temperature=0.7, top_p=0.9, num_return_sequences=1)\n",
    "    generated_text = response[0]['generated_text']\n",
    "    print(f\"Generated Text: {generated_text}\")  # Debugging: Print the generated text\n",
    "    \n",
    "    try:\n",
    "        extracted_json = json.loads(generated_text)\n",
    "        return extracted_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Process the provided CV file\n",
    "file_path = CV_PATH\n",
    "cv_text = load_docs(file_path)\n",
    "if cv_text:\n",
    "    extracted_info = extract_information(cv_text)\n",
    "    print(f\"Extracted Information: {extracted_info}\")  # Debugging: Print the extracted information\n",
    "    results = [{\"file_name\": os.path.basename(file_path), \"extracted_json\": json.dumps(extracted_info)}]\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('extracted_cv_information.csv', index=False)\n",
    "\n",
    "    print(\"Extracted information saved to extracted_cv_information.csv\")\n",
    "else:\n",
    "    print(\"Failed to extract text from the provided CV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
